{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from auxiliary_file import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset construction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, index_to_word = training_data_cnstructing([pre_processing('article1.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, index_to_word):\n",
    "        self.data = data\n",
    "        self.word_to_index = index_to_word\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        context, target_word = self.data[idx]\n",
    "        context = torch.tensor(context, dtype=torch.long)\n",
    "        target_word = torch.tensor(target_word, dtype=torch.float)\n",
    "        return context, target_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = MyDataset(training_data, index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 710, 1080,    6, 1186,  876]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataloader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5])\n",
      "torch.Size([32, 1430])\n"
     ]
    }
   ],
   "source": [
    "for context, target in dataloader:\n",
    "    print(context.shape)\n",
    "    print(target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(Word2vec, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)      \n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, context_words):\n",
    "\n",
    "        embeds = self.embed(context_words)\n",
    "        \n",
    "        embeds = torch.mean(embeds, dim=1)\n",
    "        \n",
    "        out = self.linear(embeds)\n",
    "        \n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        \n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1127.374116897583\n",
      "Epoch 2, Loss: 966.134340763092\n",
      "Epoch 3, Loss: 868.01416015625\n",
      "Epoch 4, Loss: 792.3959057331085\n",
      "Epoch 5, Loss: 727.987423658371\n",
      "Epoch 6, Loss: 671.593722820282\n",
      "Epoch 7, Loss: 620.4016768932343\n",
      "Epoch 8, Loss: 573.8380517959595\n",
      "Epoch 9, Loss: 530.8325037956238\n",
      "Epoch 10, Loss: 491.59458470344543\n",
      "Epoch 11, Loss: 455.8370735645294\n",
      "Epoch 12, Loss: 422.64382553100586\n",
      "Epoch 13, Loss: 392.467813372612\n",
      "Epoch 14, Loss: 365.31044363975525\n",
      "Epoch 15, Loss: 340.1634204387665\n",
      "Epoch 16, Loss: 317.73384296894073\n",
      "Epoch 17, Loss: 297.3810614347458\n",
      "Epoch 18, Loss: 278.67775189876556\n",
      "Epoch 19, Loss: 261.91796839237213\n",
      "Epoch 20, Loss: 246.4845970273018\n",
      "Epoch 21, Loss: 232.20951998233795\n",
      "Epoch 22, Loss: 218.97701162099838\n",
      "Epoch 23, Loss: 207.40553033351898\n",
      "Epoch 24, Loss: 195.84162068367004\n",
      "Epoch 25, Loss: 185.61240375041962\n",
      "Epoch 26, Loss: 175.93339723348618\n",
      "Epoch 27, Loss: 166.8773576617241\n",
      "Epoch 28, Loss: 158.21355086565018\n",
      "Epoch 29, Loss: 150.66912174224854\n",
      "Epoch 30, Loss: 143.17963576316833\n",
      "Epoch 31, Loss: 135.98357099294662\n",
      "Epoch 32, Loss: 129.62640699744225\n",
      "Epoch 33, Loss: 123.57753017544746\n",
      "Epoch 34, Loss: 117.91346663236618\n",
      "Epoch 35, Loss: 112.50452297925949\n",
      "Epoch 36, Loss: 107.23618510365486\n",
      "Epoch 37, Loss: 102.94961977005005\n",
      "Epoch 38, Loss: 98.15295240283012\n",
      "Epoch 39, Loss: 94.21304696798325\n",
      "Epoch 40, Loss: 90.10351201891899\n",
      "Epoch 41, Loss: 86.53126657009125\n",
      "Epoch 42, Loss: 83.25168269872665\n",
      "Epoch 43, Loss: 80.11809307336807\n",
      "Epoch 44, Loss: 76.86206805706024\n",
      "Epoch 45, Loss: 73.9114282131195\n",
      "Epoch 46, Loss: 71.23256213963032\n",
      "Epoch 47, Loss: 68.841055393219\n",
      "Epoch 48, Loss: 66.49027779698372\n",
      "Epoch 49, Loss: 64.21672786772251\n",
      "Epoch 50, Loss: 62.20892991125584\n",
      "Epoch 51, Loss: 60.020850613713264\n",
      "Epoch 52, Loss: 58.13974487781525\n",
      "Epoch 53, Loss: 56.321937903761864\n",
      "Epoch 54, Loss: 54.734796196222305\n",
      "Epoch 55, Loss: 53.341306030750275\n",
      "Epoch 56, Loss: 51.60205526649952\n",
      "Epoch 57, Loss: 50.280742675065994\n",
      "Epoch 58, Loss: 48.77918355166912\n",
      "Epoch 59, Loss: 47.58322465419769\n",
      "Epoch 60, Loss: 46.43068455159664\n",
      "Epoch 61, Loss: 45.39671537280083\n",
      "Epoch 62, Loss: 44.17474013566971\n",
      "Epoch 63, Loss: 43.10996063053608\n",
      "Epoch 64, Loss: 42.10991683602333\n",
      "Epoch 65, Loss: 41.12414059787989\n",
      "Epoch 66, Loss: 40.34453955292702\n",
      "Epoch 67, Loss: 39.44619246572256\n",
      "Epoch 68, Loss: 38.70021065324545\n",
      "Epoch 69, Loss: 37.7631653919816\n",
      "Epoch 70, Loss: 37.010335862636566\n",
      "Epoch 71, Loss: 36.30983743816614\n",
      "Epoch 72, Loss: 35.65280383825302\n",
      "Epoch 73, Loss: 34.990842528641224\n",
      "Epoch 74, Loss: 34.58343780040741\n",
      "Epoch 75, Loss: 33.93681153655052\n",
      "Epoch 76, Loss: 33.31147304922342\n",
      "Epoch 77, Loss: 32.88493550568819\n",
      "Epoch 78, Loss: 32.409595757722855\n",
      "Epoch 79, Loss: 31.79321739077568\n",
      "Epoch 80, Loss: 31.361590303480625\n",
      "Epoch 81, Loss: 30.752255156636238\n",
      "Epoch 82, Loss: 30.354728139936924\n",
      "Epoch 83, Loss: 30.014032065868378\n",
      "Epoch 84, Loss: 29.51823790371418\n",
      "Epoch 85, Loss: 29.28056576102972\n",
      "Epoch 86, Loss: 28.837554164230824\n",
      "Epoch 87, Loss: 28.363092809915543\n",
      "Epoch 88, Loss: 28.117304913699627\n",
      "Epoch 89, Loss: 27.810124672949314\n",
      "Epoch 90, Loss: 27.493690714240074\n",
      "Epoch 91, Loss: 27.09661716222763\n",
      "Epoch 92, Loss: 26.772211775183678\n",
      "Epoch 93, Loss: 26.603759855031967\n",
      "Epoch 94, Loss: 26.101497165858746\n",
      "Epoch 95, Loss: 25.85760237276554\n",
      "Epoch 96, Loss: 25.634466029703617\n",
      "Epoch 97, Loss: 25.49428366869688\n",
      "Epoch 98, Loss: 25.094763658940792\n",
      "Epoch 99, Loss: 24.89115370064974\n",
      "Epoch 100, Loss: 24.701302282512188\n",
      "Epoch 101, Loss: 24.53332694619894\n",
      "Epoch 102, Loss: 24.020006585866213\n",
      "Epoch 103, Loss: 24.05785048007965\n",
      "Epoch 104, Loss: 23.84920947626233\n",
      "Epoch 105, Loss: 23.583689853549004\n",
      "Epoch 106, Loss: 23.29657046496868\n",
      "Epoch 107, Loss: 23.246670357882977\n",
      "Epoch 108, Loss: 23.03485455736518\n",
      "Epoch 109, Loss: 22.89566595479846\n",
      "Epoch 110, Loss: 22.47129040211439\n",
      "Epoch 111, Loss: 22.317555096000433\n",
      "Epoch 112, Loss: 22.26649060472846\n",
      "Epoch 113, Loss: 22.17301056906581\n",
      "Epoch 114, Loss: 21.856967754662037\n",
      "Epoch 115, Loss: 21.894966423511505\n",
      "Epoch 116, Loss: 21.565880075097084\n",
      "Epoch 117, Loss: 21.607051391154528\n",
      "Epoch 118, Loss: 21.410690929740667\n",
      "Epoch 119, Loss: 21.22170640528202\n",
      "Epoch 120, Loss: 21.089370626956224\n",
      "Epoch 121, Loss: 20.88805589824915\n",
      "Epoch 122, Loss: 20.740791704505682\n",
      "Epoch 123, Loss: 20.60059878975153\n",
      "Epoch 124, Loss: 20.372816864401102\n",
      "Epoch 125, Loss: 20.387338049709797\n",
      "Epoch 126, Loss: 20.316053234040737\n",
      "Epoch 127, Loss: 20.29901436716318\n",
      "Epoch 128, Loss: 19.90474170073867\n",
      "Epoch 129, Loss: 19.88179027661681\n",
      "Epoch 130, Loss: 19.721995908766985\n",
      "Epoch 131, Loss: 19.624619614332914\n",
      "Epoch 132, Loss: 19.699814174324274\n",
      "Epoch 133, Loss: 19.432926654815674\n",
      "Epoch 134, Loss: 19.504416525363922\n",
      "Epoch 135, Loss: 19.382921878248453\n",
      "Epoch 136, Loss: 19.351633358746767\n",
      "Epoch 137, Loss: 18.95482286810875\n",
      "Epoch 138, Loss: 19.034300331026316\n",
      "Epoch 139, Loss: 18.924145326018333\n",
      "Epoch 140, Loss: 18.781000316143036\n",
      "Epoch 141, Loss: 18.755738601088524\n",
      "Epoch 142, Loss: 18.720414239913225\n",
      "Epoch 143, Loss: 18.815041664987803\n",
      "Epoch 144, Loss: 18.49599364772439\n",
      "Epoch 145, Loss: 18.34465604647994\n",
      "Epoch 146, Loss: 18.364869698882103\n",
      "Epoch 147, Loss: 18.178540535271168\n",
      "Epoch 148, Loss: 18.307588692754507\n",
      "Epoch 149, Loss: 17.914498046040535\n",
      "Epoch 150, Loss: 18.087340593338013\n",
      "Epoch 151, Loss: 17.82366770133376\n",
      "Epoch 152, Loss: 18.08067363500595\n",
      "Epoch 153, Loss: 17.89939646050334\n",
      "Epoch 154, Loss: 17.77956335619092\n",
      "Epoch 155, Loss: 17.80484757758677\n",
      "Epoch 156, Loss: 17.63813815265894\n",
      "Epoch 157, Loss: 17.78253387287259\n",
      "Epoch 158, Loss: 17.677454816177487\n",
      "Epoch 159, Loss: 17.495663851499557\n",
      "Epoch 160, Loss: 17.418159898370504\n",
      "Epoch 161, Loss: 17.206590827554464\n",
      "Epoch 162, Loss: 17.266173005104065\n",
      "Epoch 163, Loss: 17.031372979283333\n",
      "Epoch 164, Loss: 17.10870862752199\n",
      "Epoch 165, Loss: 17.17232885211706\n",
      "Epoch 166, Loss: 16.92234742641449\n",
      "Epoch 167, Loss: 16.958648903295398\n",
      "Epoch 168, Loss: 16.971745250746608\n",
      "Epoch 169, Loss: 16.693236220628023\n",
      "Epoch 170, Loss: 16.863306989893317\n",
      "Epoch 171, Loss: 16.703729428350925\n",
      "Epoch 172, Loss: 16.544751415029168\n",
      "Epoch 173, Loss: 16.60098733007908\n",
      "Epoch 174, Loss: 16.752910932525992\n",
      "Epoch 175, Loss: 16.638091519474983\n",
      "Epoch 176, Loss: 16.5669603087008\n",
      "Epoch 177, Loss: 16.41322737187147\n",
      "Epoch 178, Loss: 16.52274825423956\n",
      "Epoch 179, Loss: 16.361538264900446\n",
      "Epoch 180, Loss: 16.261636082082987\n",
      "Epoch 181, Loss: 16.323503157123923\n",
      "Epoch 182, Loss: 16.311510898172855\n",
      "Epoch 183, Loss: 16.37491610273719\n",
      "Epoch 184, Loss: 16.19221381470561\n",
      "Epoch 185, Loss: 15.949299978092313\n",
      "Epoch 186, Loss: 16.038650242611766\n",
      "Epoch 187, Loss: 15.912260077893734\n",
      "Epoch 188, Loss: 15.903020495548844\n",
      "Epoch 189, Loss: 15.897024320438504\n",
      "Epoch 190, Loss: 15.938157206401229\n",
      "Epoch 191, Loss: 15.895912639796734\n",
      "Epoch 192, Loss: 15.766652807593346\n",
      "Epoch 193, Loss: 15.812593316659331\n",
      "Epoch 194, Loss: 15.767293183133006\n",
      "Epoch 195, Loss: 15.775410460308194\n",
      "Epoch 196, Loss: 15.667470103129745\n",
      "Epoch 197, Loss: 15.726547690108418\n",
      "Epoch 198, Loss: 15.685343375429511\n",
      "Epoch 199, Loss: 15.630532654002309\n",
      "Epoch 200, Loss: 15.389960955828428\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(data_set[0][1])\n",
    "embed_size = 100\n",
    "num_epochs = 200\n",
    "\n",
    "model = Word2vec(vocab_size, embed_size)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.9)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for context, target_word in dataloader:\n",
    "\n",
    "        log_probs = model(context)\n",
    "        \n",
    "        loss = loss_function(log_probs, torch.argmax(target_word, dim=1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.embed.weight.data\n",
    "\n",
    "word_vectors = {}\n",
    "\n",
    "for index, word in data_set.word_to_index.items():\n",
    "\n",
    "    word_vectors[word] = embeddings[index].numpy()\n",
    "\n",
    "vector = word_vectors['whole']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08504745, -0.06208043, -0.13924137,  0.06137526,  0.03140182,\n",
       "       -0.04903676, -0.06061382, -0.09932429,  0.05973937,  0.12183657,\n",
       "       -0.03803857,  0.10438314, -0.0643212 , -0.04555834,  0.0886331 ,\n",
       "       -0.02583544,  0.0176372 ,  0.06187674,  0.1003326 , -0.18175004,\n",
       "        0.02577518,  0.01620501,  0.07904318,  0.02467192, -0.05901472,\n",
       "        0.02251715,  0.05450393, -0.01737169, -0.15636475,  0.05712589,\n",
       "        0.01381911,  0.04989778, -0.09927873,  0.21000224,  0.03257312,\n",
       "       -0.13371833, -0.05527603,  0.00366283, -0.09176171, -0.02369864,\n",
       "        0.05874272,  0.01988603, -0.08102527, -0.07479553, -0.02615019,\n",
       "       -0.0096089 , -0.03722088,  0.00905608, -0.00557863, -0.06213749,\n",
       "       -0.03447596,  0.1705859 , -0.05792461, -0.08617669, -0.0273452 ,\n",
       "       -0.057104  , -0.05722999,  0.09842712, -0.1318504 , -0.05002528,\n",
       "       -0.1026495 ,  0.07031095,  0.04980109,  0.03997704,  0.02542106,\n",
       "       -0.00435334, -0.02236769, -0.04254798, -0.0405719 ,  0.00076974,\n",
       "       -0.06510094, -0.06350776, -0.07136376, -0.11826488, -0.01310173,\n",
       "        0.17839648, -0.12481134,  0.00103645, -0.07007927, -0.01685527,\n",
       "        0.01091597, -0.02283773,  0.01129218,  0.04904772,  0.04207213,\n",
       "       -0.04262587, -0.15022388,  0.09717921, -0.04324098,  0.07185239,\n",
       "        0.06545737, -0.00635332,  0.02342416, -0.07897893, -0.02625746,\n",
       "        0.08782927,  0.00256907,  0.006525  ,  0.04229377, -0.00172814],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector(word_vectors, pre_processing('article1.txt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
