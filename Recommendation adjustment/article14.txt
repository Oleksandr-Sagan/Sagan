Voice EHR: Introducing Multimodal Audio Data for Health James Anibal1,2, Hannah Huth1, Ming Li1, Lindsey Hazen1, Yen Minh Lam3, Nguyen Thi Thu Hang3, Michael Kleinman4*, Shelley Ost4*, Christopher Jackson4*, Laura Sprabery4*, Cheran Elangovan4*, Balaji Krishnaiah4*, Lee Akst5,6*, Ioan Lina6*, Iqbal Elyazar7*, Lenny Ekwati7*, Stefan Jansen8*, Richard Nduwayezu9*, Charisse Garcia1, Jeffrey Plum1, Jacqueline Brenner1, Miranda Song1, Emily Ricotta11,12, David Clifton2, C. Louise Thwaites3, Yael Bensoussan13, Bradford Wood1  1Center for Interventional Oncology, NIH Clinical Center, National Institutes of Health, USA 2 Computational Health Informatics Lab, Oxford Institute of Biomedical Engineering, University of Oxford, UK 3Oxford University Clinical Research Unit, Ho Chi Minh City, Vietnam 4College of Medicine, University of Tennessee Health Sciences Center, Memphis, Tennessee, USA 5Johns Hopkins Voice Center, Johns Hopkins University, Baltimore, MD, USA 6Department of Otolaryngology-Head and Neck Surgery, Johns Hopkins University School of Medicine, Baltimore, Maryland, USA 7Oxford University Clinical Research Unit Indonesia, Jakarta, Indonesia 8College of Medicine and Health Sciences, University of Rwanda, Kigali, Rwanda 9King Faisal Hospital, Kigali, Rwanda 11Epidemiology and Data Management Unit, National Institute of Allergy and Infectious Diseases, Bethesda, MD, USA 12Department of Preventive Medicine and Biostatistics, Uniformed Services University 13Morsani College of Medicine, University of South Florida, Tampa Bay, Florida, USA *Contributed equally   Abstract                Large AI models trained on audio data may have the potential to rapidly classify patients, enhancing medical decision-making and potentially improving outcomes through early detection. Existing technologies depend on limited datasets using expensive recording equipment in high-income, English-speaking countries. This challenges deployment in resource-constrained, high-volume settings where audio data may have a profound impact. This report introduces a novel data type and a corresponding collection system that captures health data through guided questions using only a mobile/web application. This application ultimately results in an audio electronic health record (“voice EHR”) which may contain complex biomarkers of health from conventional voice/respiratory features, speech patterns, and language with semantic meaning - compensating for the typical limitations of unimodal clinical datasets. This report introduces a consortium of partners for global work, presents the application used for data collection, and showcases the potential of informative voice EHR to advance the scalability and diversity of audio AI.   1. Introduction                                                                                                 The recent COVID-19 pandemic has underscored the limitations of healthcare systems globally and highlighted the need for data innovations to support both care providers and patients. The high volume of patients seeking medical care for COVID-19 and other respiratory infections has caused extraordinary challenges for healthcare systems. These include long waitlists, limited time for each patient, increased testing costs, exposure risks for healthcare workers, and documentation burdens.1 Adding to the problem, the world is facing nursing and physician shortages which are expected to rise dramatically over the next 10 years.2,3,4 This only contributes to the rapidly increasing rates of healthcare worker burnout, which has been particularly severe since the onset of the COVID-19 pandemic.5,6  The overwhelming burden on the medical field prevents many patients from scheduling an appointment with a nearby physician. This subsequently introduces logistical difficulties in requesting time away from employment, finding childcare, and/or arranging transportation to a clinic.7,8 Decreased engagement with the healthcare system often results in patients perceiving a diminished sense of empathy from their providers. This was illustrated in a recent study which found that 80% of participants in an online health forum preferred answers from ChatGPT compared to physicians, both in terms of quality of response and empathy.9 Perceived and actual barriers to effective patient-provider communication likely reduce the quality of care and may ultimately worsen patient outcomes. As such, innovative digital strategies are imperative to improve patient-provider access while reducing pressures on the primary care field.  Artificial intelligence (AI) has been proposed as a possible mechanism to perform key clinical tasks such as diagnostics, triage, and patient monitoring in the clinic and at home. In the past decade, AI has been refined and rapidly adopted across a multitude of industries; yet, despite many attempts, AI has failed to meaningfully impact the trajectory of recent global health crises like the COVID-19 pandemic.10 Still, AI-driven diagnosis and decision support are major focus areas within digital health research. This is particularly true with the advent of the GPT models and other multimodal large language models (LLMs), which have unprecedented capabilities in question answering, image interpretation, programming, and other complex tasks.11,12 As a result, technology companies have begun to develop foundation models for the healthcare space. However, these are mainly designed for processing and diagnostic tasks with privileged data (e.g., images) or as a Chatbot-styled tool for question-answering rather than providing specific recommendations from patient data.13,14 While future LLMs may provide promising augmentations to the healthcare system, serious data challenges do remain for the widespread, equitable deployment of AI models in healthcare. Here, several primary obstacles are outlined:  Data Availability and Interoperability: In many cases, clinical AI models require correlated data – information from different sources related to the same patient in the same approximate period of time. Datasets also require extensive curation, which is often expensive, inconvenient, and frequently overlooked as a challenge in the development of AI for health. Multimodal data must be linked from across disjointed sources/centers, which often have incompatible systems and vastly different regulatory structures. Excluding underserved groups: Currently, many AI technologies are dependent on the availability, quality, and breadth of data in electronic records, which are often unavailable or inaccurate in many settings, particularly in resource constrained areas such as low- and middle- income countries (LMICs) or extremely rural areas in high-income countries like the United States. These disparities are due to a wide range of factors, which include biased allocation of diagnostic tests and other healthcare services, gaps in insurance coverage which reduces access to premium resources, and other barriers (e.g., transportation) due to a lack of providers or facilities. As a result, data used to train AI models is often biased against underserved populations.15 Misalignment with clinical processes:  The data collected in current clinical workflows is incompatible with most AI systems, causing development challenges and hesitancy from healthcare workers, who make decisions based on patient reporting, their own observations, and various tests - not narrow unimodal datasets.  Contributions                                  In response to these problems, this work makes the following contributions to the structure and collection of healthcare data: 1. Development of a mobile/web application (Healthcare via Electronic and Acoustic Records, “HEAR”) to facilitate the collection of customized multimodal data (text-sound pairs) for accessible AI models. The application is designed to be patient-facing, intuitive, potentially useful for healthcare workers, and technically lightweight for deployment in low-connectivity areas. This system simultaneously captures patient-reported health information (via recorded speech) and unique variations in sound data (changes in voice/speech) present in the recording. The HEAR application facilitates fast collection of health data, including retrospective context, in a single setting, rather than requiring the user to type large amounts of text into a lengthy form which causes respondent fatigue and results in data with decreasing accuracy and increasing sparsity. 16,17  Initial studies will focus on respiratory illnesses, neurological conditions, voice/speech disorders, selected cancers, and healthy controls. A complete list of illnesses can be found on the application (web-based prototype version: www.hearai.org ).  2. Presentation of case studies from an initial, limited sample population, demonstrating potential viability of voice EHR data collected across multiple settings, encouraging future participation in the project. This work represents a first step towards developing “large language and sound” AI models to perform clinical tasks with multimodal audio data.  2. Related Work Over the past decade, audio data has shown potential as a diagnostic tool. The idea that patients with certain conditions might present with unique changes in their voice before showing more progressive signs of disease largely originated with Parkinson’s disease. Multiple studies have shown that Parkinson’s disease is associated with characteristic and progressive changes in phonation over the disease course, including speech biomarkers such as decreased word stress, softened consonants, abnormal silences, and monotone speech.18-21 Similarly, many studies have since identified specific voice changes in patients with asthma, COPD, interstitial lung disease, rheumatoid arthritis, chronic pain, diabetes, and cancer.22-27  The formation of the Bridge2AI Voice AI Consortium shows the increasing interest in leveraging voice data as a low-cost and freely accessible data modality for healthcare usage.28 With the rapid advancement of AI, there is a unique opportunity to detect respiratory, neurodegenerative, and even malignant disease processes through audio AI models.   During the COVID-19 pandemic, the demand for remote healthcare solutions surged, providing an ideal setting to advance audio AI technology. Specifically, several methods were developed which allowed remote patients to predict their COVID-19 status (positive/negative) or variant status via machine learning models trained to detect differences in acoustic features or spoken words contained recorded audio. 29-36 However, many of these efforts were not deployed due to one or several of the following limitations related to the training datasets:   1. Dataset Size/Diversity: Many voice studies referenced here are reliant on small datasets collected from a narrow range of English-speaking patients, preventing broad deployment in hospitals or at-home settings.     2. Data Quality: Many studies were built around on crowdsourced datasets, which face significant issues with data quality – reliable annotations (specific indications of disease or health state) are difficult to achieve when collecting limited unimodal data from a huge range of possible environments and devices. 35-38 Many of the data points, which contain only scripted voice samples, will not be usable due to the lack of context needed to account for these sources of noise. Moreover, none of these datasets were curated through partnerships with healthcare worker and, as such, do not actually confirm diagnosis of COVID or other illnesses.  3. Data Breadth: Past audio AI studies, particularly those involving COVID-19, were often designed to separate between perfectly healthy samples and COVID-19 positive samples.35 This excludes a key COVID-19 negative cohort: patients with other respiratory illnesses. A typical user of any testing method would do so because of symptoms, which may then confuse an AI model trained only to separate between one disease and fully healthy controls. More specifically, although COVID-19 can cause laryngitis and inflammation of the vocal cords causing voice changes many other factors can cause laryngitis leading to voice changes such as smoking, infections, or other environmental factors.35    This study introduces “voice EHR" – patients share their past medical history and progression of present illness through audio recordings, creating a patient-driven temporal record of clinical information to contextualize breathing, voice, and speech data collected simultaneously.   3.  Methods                                                                                                                                 The development of AI models to accurately detect audio biomarkers and match them with descriptions of associated disease process is dependent on the acquisition of robust training datasets from a range of settings. The proposed “voice EHR” methods are designed to enable semantic representations of clinical data containing approximate temporal context (e.g., change from baseline health, described results of lab testing or imaging studies) with correlated samples of audio data: voice/breathing sounds and speech patterns. This data collection paradigm, termed “tell me about your health”, may facilitate the acquisition of multimodal audio data at scale.  This protocol has been approved by the Institutional Review Board of the U.S National Institutes of Health (NIH). Informed consent will be obtained from all participations prior to data collection, using a built-in consent form on the data collection application. Data is stored on NIH-secured cloud servers maintained by Amazon Web Services (AWS).39 To ensure data anonymization, no identifying information will be stored at this time. While this increases participant privacy, it prohibits linking of repeated data entry episodes by the same person. In future, linking of data may occur by matching voice prints and other non-PII information or through the implementation of a longitudinal voice EHR system with privacy permissions controlled by participants.  3.1 Participant Recruitment and Study Population  The data collection process is deployed through two primary channels: 1) public use of the application, which is available online, and 2) partnerships with healthcare professionals working at collaborating point-of care settings, including telehealth apps/platforms. Specific recruiting efforts have been designed to recruit clinics in LMICs and resource-constrained areas/healthcare deserts. The HEAR app is low-cost, low-bandwidth, fast/easy to use, and does not rely on any specific expensive technologies (e.g., sound booths, MRI machines), which facilitates partnerships with healthcare workers in such settings, in contrast to many AI-focused studies which are run at major hospitals. Collaboration with healthcare professionals will help improve the reliability of voice EHR data by providing validated labels through recruitment of patients with confirmed diagnoses of interest. The simple, straightforward application can be used broadly within a healthcare ecosystem - by physicians, physician’s assistants, nurses, technicians, clinical researchers, and students/trainees.   3.2 Mobile Application The HEAR application is designed to efficiently collect multimodal audio data for health – voice EHR – via a combination of short survey questions and recorded voice/speech/breathing. Interested patients seen at a participating location will be instructed on proper application usage by a member of the clinical staff and then will submit their voice data while waiting for their provider. Detailed instructions are also provided on the app interface to support users in an “at-home” setting or another non-clinical environment.   3.3 Data Collection The HEAR app contains three main sections (Fig. 1 – left). After obtaining informed consent, data collection begins with multiple-choice questions for collection of basic health information (pages 1 – 5). This initial section is necessary during the data collection process to ensure a balanced training dataset for initial model development and validation but will be removed after deployment of validated technology reliant only on voice EHR audio data. Patients then provide audio data based on written instructions (pages 6 – 12). The final section (also audio data) is completed with the assistance of their care provider to document clinical findings, next steps, probable diagnosis, and other components of the appointment (pages 13 – 15).  Healthy volunteers do not complete pages 4, 8, or 13-15.   
     Figure 1. (Left) Overview of voice EHR app, including initial survey, patient audio, and information from HCWs. (Right) Screenshot of page 8 from the app (second audio prompt).   3.3.1 Audio Data This section of the report describes novel methods for collecting multimodal audio data, containing information on voice/breathing sounds and speech patterns as well as semantic meaning from spoken language. Each prompt is designed based on real-world clinical workflows, which may enable the collection of training data which is more compatible with existing healthcare systems. Table 1 contains voice prompts and a short descriptor of each. After collection, all audio recordings containing semantic meaning will be transcribed. Automated speech recognition (ASR) for this study is completed using Whisper - a foundation AI model for speech-to-text applications.40    Table 1: Participant Prompts included on the HEAR Application for audio data collection.  Prompt Description and Purpose                Completed By  1. Please tell us background information about your health before your current illness, including any physical and mental health problems, and medications. (3 min.) Establishes a baseline to contextualize changes due to illness, either in sounds, speech patterns, or spoken words.   Patients Healthy Volunteers 
 2. In as much detail as possible, please tell us how your illness has developed from the time when you first noticed symptoms until now. Include any medications you took (like Tylenol) or steps you use to reduce your symptoms. Please use words/phrases like "on the first day", "in the morning", "then", "after that" and use descriptive words like "mild", "severe". No detail is too small. (3 min.)  Captures the complaint of the patient by approximating a record of illness progression.     Patients Only 
3. Please tell us if you or anyone else has noticed any recent changes in your voice or speech. These should be changes that started around the same time as this illness episode, not any chronic long-term changes. (1 min.)  Establishes an “audio” baseline to contextualize changes in voice/speech which may arise from lifestyle factors/past conditions or be a biomarker of acute illnesses.    Patients  Healthy Volunteers  4. Part 1: Say each of these vowels for 3-5 seconds. aaaaa (as in made); eeeee (beet); ooooo (cool)  Part 2: Read these sentences: “When the sunlight strikes raindrops in the air, they act as a prism and form a rainbow. The rainbow is a division of white light into many beautiful colors. These take the shape of a long round arch, with its path high above, and its two ends apparently beyond the horizon.”  Conventional voice and respiratory data for analysis of sound changes.     Patients Healthy Volunteers  5. Part 1: Hold the device near your nose and record yourself breathing normally for 30 seconds with your mouth closed.   Part 2: Hold the device near your mouth and record yourself taking 3 deep breaths through your mouth.  Conventional respiratory data for analysis of breathing changes and determination of respiratory rate.  Patients Healthy Volunteers  6. Is there anything else you think may be affecting your health in general or your current illness that you did not already share? For example, you can tell us about your employment or lifestyle habits. (1 min)   Captures specific circumstances related to health which the patient considers to be important.   Patients Healthy Volunteers  7. Your physician or other provider should briefly describe the physical exam (given to you by the physician), any available lab results, imaging studies, the diagnosis, and other next steps related to testing, treatment, or monitoring the illness. If the healthcare provider is not available or you are at home, you can record this information yourself. (3 min.)  Audio approximation other types of multimodal data which may be key context for the patient data collected by the application.     Patients Healthcare Workers Initial Data: Demographic Information for model development             AI models developed from voice EHR data will be trained to perform clinical tasks using only the multimodal audio data. However, in the experimental stages, respondents are asked to complete an additional brief survey to contextualize the collected illness and voice data. This is done to ensure class balance, account for possible sources of bias, and run comparative experiments with traditional EHR data. These data include race, sex, symptoms, education, insurance, and health history. Zip codes or other approximate indicators of location are also collected for epidemiological studies.   Prompts Prompt 1: Health baseline                 The health baseline prompt (Table 1, prompt 1) is designed to provide background data on participant, ensuring that disease can be represented as a function of drift from a fixed point. Purely cross-sectional datasets are unrealistic, potentially misinforming clinicians as in a real-world scenario. No patient would be seen, let alone treated, before the care team reviewed the past medical records or collected past medical history. Prompt 2: Illness Trajectory                 Prompt 2 of the HEAR application is designed to mirror a key interaction between a patient and their provider: “What brings you in today?” (Table 1, prompt 2). During this interaction, detailed, temporal descriptions of illnesses and corresponding patient-initiated interventions (e.g., “I am taking Tylenol”) are collected, mirroring basic clinical assessments. The aim of this prompt is to ensure clinical information with temporal context is available to complement the sound data and account for potential sources of noise. The application asks patients to use basic terminology to describe, in chronological order, the progression of their illness with any associated signs, symptoms, complications, and corresponding interventions. Collecting this information via an audio recording is far less burdensome than a typed/written form, is likely to contain more detailed information, and may serve as an effective substitution for conventional time-series EHR data which is often sparse or unavailable, especially regarding over the counter and alternative therapies. Additionally, the use of LLMs to parse this data may address a key health communication issue: such descriptions are often overwhelming for healthcare professionals when engaging with patients.41  Prompt 3: Voice baseline                             Studies involving past sound datasets have shown the obstructive impact of confounding variables such as chronic laryngeal conditions or lifestyle factors such as smoking.35 As such, the HEAR application prompts the patient to report any recent changes in voice, speech, or breathing noticed by themselves or others (Table 1, prompt 3). As with the health baseline prompt, this prompt aims to replace baseline information in original form (e.g., voice samples from prior to illness), which may be unobtainable for ill patients. This data reduces the obstructive impact of voice sounds or speech patterns which do not correlate directly to the current complaint.  Prompt 4: Conventional Acoustic Data          Prompt 4 in Table 1 facilitates the collection of conventional acoustic data that is typically used in voice AI studies. These data (prompt 4, part 1) help assess the impact of different variables on how air flows over the vocal cords. The prompt is the simplest method of collecting this type of data, can be easily translated in other languages, and has been used in large-scale crowdsourced studies in the past.35 Prompt 4, part 2 – the “rainbow passage” – is a validated passage designed to maximize the amount of different acoustic features contained in a single data sample, ensuring that biomarkers are not missed due to limited/narrow inputs.42 These data are collected not only to ensure that pure sound samples are available alongside the free speech with semantic meaning, but also as a mechanism for interoperability and comparison with data from past studies.  Prompt 5: Conventional Breathing Data          Participants are also asked to breathe through the nose normally for 30 seconds (prompt 4, part 3) and take 3 deep, open-mouthed breaths (prompt 4, part 4), facilitating tasks such as the calculation of respiratory rate (used in continuous vital sign monitoring) and the capture of airway conditions such as stridor or distinctive alterations from supraglottic edema. 43  Prompt 6: Additional Information                                                     To further ensure that HEAR is collecting comprehensive patient-centered data about medical history and history of present illness, prompt 6 asks if the respondent has any other information that may be important to share (i.e., any contributing information that might not have been covered by past prompts). As such, voice EHR data is likely to be more broadly informative than limited sets of features from either survey forms or structured health records. This may lead to significant improvements compared to past health datasets which have been shown to have bias against underserved minority groups or individuals with unique and/or complicated clinical needs not considered in the design of EHR systems or standardized surveys.  Prompt 7: Diagnosis and Treatment Plan                                             If available/applicable, a healthcare worker will be asked to provide a brief recorded description of the diagnosis and treatment plan (Table 1, prompt 7). Components of this data will be used to approximate other types of clinical data which are often not collected and stored in low-resource settings, such as lab results. Components of these data will be also used to provide annotations for voice EHR data.   3.4 Multi-Lingual Capabilities                      Many recent multimodal LLMs (e.g., GPT-4), have been shown to have multi-lingual capabilities.11,44 As such, the HEAR app has been and will continue to be adapted for a broad range of languages other than English. Across different languages, most prompts can be translated directly; the main area requiring customization is the conventional voice data from Prompt 4. The rainbow passage and elongated vowels shown in Table 1 were selected to optimize the diversity of acoustic features from speech in English - equivalent passages and vowels are needed for each language included on the app. Table 2 shows the Vietnamese version of these prompts.45 Table 2: Comparison of Conventional Voice Data Scripts between English and Vietnamese Prompt English Version Translated Vietnamese Equivalent Elongated Vowels aaaaa (as in made) eeeee (as in beet) ooooo (as in cool) âyyyyy (as in mây) iiiiii (as in vi) uuuuuu (as in lu) Rainbow Passage “When the sunlight strikes raindrops in the air, they act as a prism and form a rainbow. The rainbow is a division of white light into many beautiful colors. These take the shape of a long round arch, with its path high above, and its two ends apparently beyond the horizon.” “Hey, buffalo, I tell you this: Buffalo, go out to the field, plow with me. Plowing is the farmer's true vocation. Here I am, there you are, who else will take care of the work. When the rice plants begin to bloom, there will still be grass in the fields for the buffalo to eat.”  4. Preliminary Results This study has resulted in the development of a lightweight, simple mobile application for the collection of multimodal audio data. These developments have led to the establishment of a multi-center initiative called the HEAR consortium, allowing healthcare organizations to be involved in the project regardless of their current resources or staff. Currently, the HEAR consortium has engaged partners in the U.S., Vietnam, Indonesia, and Rwanda, and is actively seeking new participants. This multinational initiative is designed to ensure a vast array of patient populations and diseases are present in the dataset. As a result, this study represents, to the best of our knowledge, the first attempt at health-related audio data collection for low-resource languages such as Vietnamese, Bahasa Indonesia, Javanese, and Kinyarwanda.   4.2 Case Studies of Initial Data              Examples of basic health information and voice data transcripts for three patients with respiratory illnesses and three healthy controls are presented below. This is a very limited sample of data for illustrative purposes; therefore, no modelling or predictive analysis was performed.  Table 3: Examples of Basic Health Information from the HEAR application.  Patient A Patient B Patient C Control A Control B Control C Age 40 55 74 37 61 66 Weight 175 117 152 190 250 182 Sex Male Female Female Male Male Female Racial/Ethnic Identification White White Hispanic White No Response Black/AA Occupation Physician Nurse Nurse Brewer Executive Warehouse Insurance Status Private Public Private Private Private Private Education Graduate College Graduate College Graduate College  Recording Site Home Home Hospital Hospital Hospital Hospital Health History None None Hypertension Cardiovascular disease Thyroid disease None Hypertension Diabetes Sleep disorders Depression Hypertension Thyroid Sleep disorders Symptoms Cough (NP) Sore throat Headache Runny Nose Sore Throat Productive (wet) cough Sore throat Muscle aches N/A N/A N/A Duration 3 3 3 N/A N/A N/A Progression Worsening No change Improving N/A N/A N/A  Full transcribed excerpts from each of the verbal prompts (Tables 4 – 7 below) represent the quality and quantity of information contained in voice EHR data from acutely ill patients and healthy controls.  Background Health Information                                                                                  Background health information provided by both patients and healthy controls (Table 4) exemplifies the numerous potential sources of valuable data not captured by the initial demographic data (Table 3). For example, Patient A discusses acid reflux and use of histamines, both of which may be connected to voice changes or respiratory biomarkers.46-47 Healthy control B describes low testosterone which has been widely associated with voice changes.48 Healthy control C describes a history of sleep apnea, which is also known to impact voice/speech.49-50  Table 4: Background Health Information: Transcribed voice EH from patients and controls. Prompt: Please tell us background information about your health before your illness, including past health problems and medications. Patient A “Overall, I am very healthy. I have seasonal allergies and occasional acid reflux. I do not take any regular medications other than an occasional medicine for seasonal allergies like an antihistamine or an occasional medication for acid reflux.” Patient B “I have good overall health, no chronic conditions. I do have seasonal allergies for which I take Allegra 60 milligrams twice a day.” Patient C Once in a while I will get some back pains, but I've had history of back surgery. And nerve blocks. I really don't have any other pains. Once I did have a little bit of chest pain, but the doctor had me on telemetry and nothing serious was found. And I haven't been that sick. I've been feeling well. I've gotten better. I got better from everything. I get better. Not only my health, my mental health, but my physical health. I am growing rapidly. I'm making more progress as I believe in my own condition.” Control A “I am a 37-year-old male with no major health issues have dealt with anxiety and recent years. I’ve also got what I think is inherited genetically passed down hyperlipidemia I’ve been treating for the last several years with statins. Don’t think there’s too much in my medical history .” Control B “I consider myself a fairly healthy 61-year-old. I do not smoke. I do have low testosterone. I have tinnitus high blood pressure, high cholesterol. I’m able to walk fine I do have some sleep issues but for the most part they’re handled well. ” Control C “My health is generally pretty good right now. I have sleep disorder and I’ve had stroke history. My face burns right now on the right side of my face, the entire right side of my face and neck. It burns 24/7 since I’ve had my stroke 10-20. It effects my right eye. My right hand is kind of sort of numb. You touch my right arm it burns, and the bottom of my right foot feels numb, kind of sort of numb. Again, I have sleep apnea and I hate it. I have to wear that dumb machine and sleep with it every night. I have asthma. I was, I’m trying to lose weight and I don’t like my stomach area because I’ve had stroke and for a while, I was not active and doing anything, so I got a pudge that I can’t stand. I have pretty good eyesight but generally overall my health is good. I have bad sinuses and with the weather changing the way it is, it is not helping anything. It’s cold, cool, calm, hot, and one’s body can’t take the constant change in the weather like we’re having. But mainly my health and what bothers me most about my health is the burning on my face and my sleep apnea. My hand feeling the way it is, it’s bad, but it’s something I’ve learned to live with.”  Longitudinal Illness Descriptions                                                          Verbal illness descriptions provide not only longitudinal symptom progression but also extensive use of dynamic qualifiers (“moderate”, “little bit”, “very”) that quantify severity or other subtle relationships between signs/symptoms. Additionally, we recorded several instances of patient-initiated interventions within the illness window that could potentially account for fluctuations in audio data. Examples include gargling with saline, vitamin C supplements, OTC medication, and increased hydration (Table 5).   Table 5: Example of Current Illness Information from 3 patients. Prompt: In as much detail as possible, please tell us how your illness has developed from the time when you first noticed the first symptoms until now. Include any medications you took (like Tylenol) or steps you use to reduce your symptoms. Please use words/phrases like "on the first day", "in the morning", "then", "after that" and use descriptive words like "mild", "severe". No detail is too small. Patient A “My symptoms started about three to four days ago. I started to have a slight sore throat and a mild dry cough. I also had a slight headache at that time, but it has since resolved, period. Over the next few days, I have had worsening dry cough and a mild to moderate sore throat, period. My sore throat has remained about the same, but my cough has worsened. I have not felt the need to take medications for my symptoms up to this point, other than I've tried to increase my hydration and increase my sleep.” Patient B “On day one, symptoms started in the afternoon with voice hoarseness, sinus and nasal congestion. By day two, the throat was still hoarse but also sore at this time with increased congestion and a headache. I used ibuprofen and Tylenol for the sore throat pain and the headache. On day three, I had increased congestion, both sinus and nasal, and my lymph nodes were swollen. The sore throat was worse and my voice was only at a whisper and still had a headache. I continued to use Tylenol and ibuprofen. Ibuprofen assisted with the throat pain but did not completely eliminate it. I did do a COVID test. On day three, that came back negative for COVID. Day four, pretty much the same as day three. Throat still sore, no real improvement. Headache and lots of sinus and nasal congestion." Patient C Let's start talking. Okay. My health is, I guess it's okay. I've been under the weather this week a little bit with a sore throat and with a little bit of coughing and bringing up some sputum, but it's getting much better [extracted from first prompt]. And this past week, I started with having a stuffy nose and a sore throat. And so I started taking, I thought maybe it could be related to allergies. I started taking some over-the-counter medication for day and for night for cold and flu-type symptoms. And that seemed to help. And that's about all. And I drank vitamin C. I did a little gargling with saline. And that's it.  Voice Changes                                                      Viability of voice EHR is further supported by audio data from individuals reporting vocal changes (Table 6). Patients A-B described multiple voice changes due to illness, which can be linked to conventional sound data, thereby ensuring that these changes are considered separately from irrelevant voice/speech anomalies due to lifestyle, recording quality, or other factors. Control C described seasonal voice changes due to cold sensitivity, which could be falsely predicted as an infectious disease. This information is not captured in conventional surveys or existing audio datasets.  Table 6: Examples of Self-Identified Changes in Voice from three patients. Prompt: Please tell us if you or anyone else has noticed any recent changes in your voice or speech. These should be changes that have started around the same time as your illness, not any chronic long-term changes. Patient A “My voice has become more raspy and deeper.” Patient B “I did notice a big voice change. In fact, that was the first real symptom on day one, was having a hoarse voice. By day two, it was even more hoarse. And as the day went on, that's when my throat began to get more painful. And by day three, my voice was at a complete whisper. Today is day four.” Patient C “I have not noticed any changes in my speech pattern. I'm bilingual. Sometimes  I speak in Spanish to my Spanish family and sometimes I speak in English, so I haven't had any problems.” Control A Checked box indicating no voice changes noticed. Control B Checked box indicating no voice changes noticed. Control C “I noticed the change in my voice this week. A lot of times when the weather goes from 39 to 65, 80, I mean 65, 67, 70, there is a change in my voice. It gets raspy as it is getting raspy right now due to it being very cold in this office. But my body can't take the drastic change in the temperature without affecting my voice, my silence. I'll start sneezing all over the place. And it is to me that I've noticed, and it's not just now, but it's over the years, that my voice would get raspy and hoarse. And it's only because of the weather changing.”   Other Information (Free Response)                                        The opportunity for individuals to provide additional health information (Table 7) at the conclusion of the application is designed to capture aspects of their illness they might have forgotten, expand on ideas the prior prompts have inspired, consider other potential contributing factors to their illness, or provide any other information about their health that they feel is important. For example, Patient B mentioned a time just before the illness when cleaning products evoked similar symptoms. None of healthy controls shared any additional information.  Table 7: Example Additional Health Information from three patients. Prompt:  Is there anything else you think may be affecting your health that you would like us to know? For example, you can tell us about your employment or your lifestyle habits. Patient A Checked box indicating there is nothing else they would like to share. Patient B “On day one I was in a home where there was a cleaning lady and when I first walked in the smell of the cleaning product was so strong that I instantly started to cough and felt some issues.” Patient C Checked box indicating there is nothing else they would like to share. 5. Discussion                                   The creation of a novel “voice EHR” system introduces numerous potential benefits to the healthcare data space, including improvement of overall AI model performance and reduction of bias towards underrepresented minority groups.   5.1 Safe and Realistic Training Data for Clinical AI              The use of voice EHR as training data for models may overcome multiple barriers to the safe deployment of AI tools for low-resource or low-access settings. In under-resourced settings, traditional EHRs are often incomplete, incorrect, or “low-tech”, which disadvantages patients who may be given care based on EHR-driven AI technologies developed in high-income settings. While conventional, gold-standard annotations like lab results are not collected, prompts which were co-designed by healthcare workers and direct data collection partnerships with clinics will help ensure the viability of voice EHR. The HEAR application will facilitate the rapid collection of “voice EHR” data in a user-friendly way, without 1) requiring time-consuming and error-prone text data entry on the part of the individual, and 2) enforcing rigid, pre-defined data schema found in traditional EHR, thereby ensuring the incorporation of all information which the patient considers to be important. Furthermore, the process of creating a “voice EHR” may be useful to healthcare workers. Transcribed audio may serve as an accompaniment to clinical notes, reducing the redundancy often associated with data collection and potentially enhancing clinical workflows. In future iterations, the physician may also have the opportunity to listen to audio or read transcribed audio recordings, auditing interactions to improve care.   Voice EHR may additionally compensate for sources of confusion that are often found in clinical data. Examples of this could include lapses in patient memory, incomplete notes from healthcare workers, information reported or understood incorrectly, or information reported in colloquial terminology. Moreover, the use of voice/sound data in combination with recorded health information may capture comprehensive representations of diseases with diverse phenotypes. Sound data may contribute additional biomarkers for certain diseases, which would not currently be captured in clinician notes. Even if participants provide incoherent voice data in terms of semantic meaning, the HEAR application still captures voice and breathing data which can independently contribute to the robustness of the data. Ultimately, this self-reported multimodal audio data significantly expands upon basic health information traditionally used for digital health systems and may allow AI models to better consider chronic conditions, noticeable voice changes, speech patterns, word choices indicating mood/sentiment, potential exposures, behavioral influences, and specific disease progression.   A secondary advantage of voice EHR is the potential to leverage control data more effectively for additional studies on audio biomarkers.  For the healthy control participants, the voice EHR contained data on chronic conditions (sleep disorders, tinnitus, sinus problems, others) which could be used as annotations for studies exploring the use of audio biomarkers for at-home monitoring of long-term health challenges, expanding the role of health AI without spending significant resources on separate efforts.    5.2. Future Work                    In addition to building AI models with voice EHR data (Fig. 2), we aim to expand the study to ensure maximum clinical utility of the work. In the future, data collection may be expanded to include other modalities, including patient-submitted smartphone images (e.g., of a skin rash) and waveform recordings from smartwatches or other sensors. New wearable technologies have made this ambitious objective increasingly feasible. As foundation AI models become more capable of parsing extremely long inputs, highly multimodal correlated data has the potential to add significant value to automated systems for clinical workflow optimization, predictive tasks, or decision-making. Moreover, a privacy-aware, patient-controlled option to create a time-series voice EHR (via a username or other identifier) may be introduced to collect personalized healthy control data from each participant and to run longitudinal studies.  
  Figure 2: Hypothetical workflow of a proposed “large language and sound” model for performing clinical tasks with voice EHR data  6. Conclusion This report shows the potential of multimodal audio data for health as a safe, private, and equitable foundation for new AI models. Voice EHR may offer a proxy for detailed EHR data only found in high-resource areas, while simultaneously providing voice, speech, and respiratory data to compliment subjective patient-reported information. Ultimately, AI models trained on voice EHR may be used in the clinic and home, supporting patients in hospital “deserts” where healthcare is not readily accessible. While challenges remain, this work highlights the rich information potentially contained in voice EHR and introduces a consortium to create technology which may facilitate global health equity.  
References 1. Smyrnakis, Emmanouil, et al. "Primary care professionals’ experiences during the first wave of the COVID-19 pandemic in Greece: a qualitative study." BMC Family Practice 22.1 (2021): 1-10. 2. https://www.goodrx.com/healthcare-access/research/healthcare-deserts-80-percent-of-country-lacks-adequate-healthcare-access  3. Zhang, Xiaoming, et al. "Physician workforce in the United States of America: forecasting nationwide shortages." Human resources for health 18.1 (2020): 1-9. 4. Hoyler, Marguerite, et al. "Shortage of doctors, shortage of data: a review of the global surgery, obstetrics, and anesthesia workforce literature." World journal of surgery 38 (2014): 269-280. 5. Shin, Philip, et al. "Time out: the impact of physician burnout on patient care quality and safety in perioperative medicine." The Permanente Journal 27.2 (2023): 160. 6. Ortega, Marcus V., et al. "Patterns in physician burnout in a stable-linked cohort." JAMA Network Open 6.10 (2023): e2336745-e2336745. 7. Oluyede, Lindsay, et al. "Addressing transportation barriers to health care during the COVID-19 pandemic: Perspectives of care coordinators." Transportation Research Part A: Policy and Practice 159 (2022): 157-168. 8. Syed, Samina T., Ben S. Gerber, and Lisa K. Sharp. "Traveling towards disease: transportation barriers to health care access." Journal of community health 38 (2013): 976-993 9.  Ayers, John W., et al. "Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum." JAMA internal medicine (2023). 10. Heaven, Will Douglas. "Hundreds of AI tools have been built to catch covid. None of them helped." MIT Technology Review. Retrieved October 6 (2021): 2021. 11. OpenAI, R. "Gpt-4 technical report. arxiv 2303.08774." View in Article 2 (2023): 13. 12. Touvron, Hugo, et al. "Llama 2: Open foundation and fine-tuned chat models." arXiv preprint arXiv:2307.09288 (2023). 13. Li, Chunyuan, et al. "Llava-med: Training a large language-and-vision assistant for biomedicine in one day." Advances in Neural Information Processing Systems 36 (2024). 14. https://sites.research.google/med-palm/  15. Jayatilleke, Kushlani. "Challenges in implementing surveillance tools of high-income countries (HICs) in low middle income countries (LMICs)." Current treatment options in infectious diseases 12 (2020): 191-201.16. Tracy, John M., et al. "Investigating voice as a biomarker: deep phenotyping methods for early detection of Parkinson's disease." Journal of biomedical informatics 104 (2020): 103362. 16. Le, Austin, Benjamin H. Han, and Joseph J. Palamar. "When national drug surveys “take too long”: An examination of who is at risk for survey fatigue." Drug and alcohol dependence 225 (2021): 108769.  17. Jeong, Dahyeon, et al. "Exhaustive or exhausting? Evidence on respondent fatigue in long surveys." Journal of Development Economics 161 (2023): 102992.  18. Suppa, Antonio, et al. "Voice in Parkinson's disease: a machine learning study." Frontiers in Neurology 13 (2022): 831428. 19. Tougui, Ilias, Abdelilah Jilbab, and Jamal El Mhamdi. "Machine learning smart system for Parkinson disease classification using the voice as a biomarker." Healthcare Informatics Research 28.3 (2022): 210-221. 20. Fagherazzi, Guy, et al. "Voice for health: the use of vocal biomarkers from research to clinical practice." Digital biomarkers 5.1 (2021): 78-88. 21. Chintalapudi, Nalini, et al. "Voice Biomarkers for Parkinson's Disease Prediction Using Machine Learning Models with Improved Feature Reduction Techniques." Journal of Data Science and Intelligent Systems (2023). 22. Asim Iqbal, Mohammed, Krishnamoorthy Devarajan, and Syed Musthak Ahmed. "An optimal asthma disease detection technique for voice signal using hybrid machine learning technique." Concurrency and Computation: Practice and Experience 34.11 (2022): e6856. 23. Idrisoglu, Alper, et al. "COPDVD: Automated Classification of Chronic Obstructive Pulmonary Disease on a New Developed and Evaluated Voice Dataset." Available at SSRN 4713043 (2024). 24. Raju, Nidhin, D. Peter Augustine, and J. Chandra. "A Novel Artificial Intelligence System for the Prediction of Interstitial Lung Diseases." SN Computer Science 5.1 (2024): 143. 25. Borna, Sahar, et al. "A Review of Voice-Based Pain Detection in Adults Using Artificial Intelligence." Bioengineering 10.4 (2023): 500. 26. Saghiri, Mohammad Ali, Anna Vakhnovetsky, and Julia Vakhnovetsky. "Scoping review of the relationship between diabetes and voice quality." Diabetes Research and Clinical Practice 185 (2022): 109782. 27. Bensoussan, Yael, et al. "Artificial intelligence and laryngeal cancer: from screening to prognosis: a state of the art review." Otolaryngology–Head and Neck Surgery 168.3 (2023): 319-329. 28. Bensoussan, Yaël, Olivier Elemento, and Anaïs Rameau. "Voice as an AI Biomarker of Health—Introducing Audiomics." JAMA Otolaryngology–Head & Neck Surgery (2024). 29. Ritwik, K. V. S., Kalluri, S. B., & Vijayasenan, D. COVID-19 Patient Detection from Telephone Quality Speech Data. Preprint at arXiv https://doi.org/10.48550/ARXIV.2011.04299 (2020). 30. Usman, Mohammed, et al. "Speech as a Biomarker for COVID-19 Detection Using Machine Learning." Computational Intelligence and Neuroscience 2022 (2022). 31. Verde, L. et al. Exploring the Use of Artificial Intelligence Techniques to Detect the Presence of Coronavirus Covid-19 Through Speech and Voice Analysis. IEEE Access 9, 65750–65757 (2021). 32. Verde, L., de Pietro, G., & Sannino, G. Artificial Intelligence Techniques for the Non-invasive Detection of COVID-19 Through the Analysis of Voice Signals. Arabian Journal for Science and Engineering https://doi.org/10.1007/s13369-021-06041-4 (2021). 33. Bhattacharya, Debarpan, et al. "Analyzing the impact of SARS-CoV-2 variants on respiratory sound signals." arXiv preprint arXiv:2206.12309 (2022). 34. Alkhodari, M., & Khandoker, A. H. Detection of COVID-19 in smartphone-based breathing recordings: A pre-screening deep learning tool. PLOS ONE 17(1), 1–25 (2022). 35. Han, J. et al. (2022). Sounds of COVID-19: exploring realistic performance of audio-based digital testing. Npj Digital Medicine 5(1), 16 (2022). 36. Anibal, James, et al. “Omicron detection with large language models and YouTube audio data”. Medrxiv preprint medrxiv: 2022.09.13.22279673 (2024).  37. Bhattacharya, Debarpan, et al. "Coswara: A respiratory sounds and symptoms dataset for remote screening of SARS-CoV-2 infection." Scientific Data 10.1 (2023): 397. 38. Triantafyllopoulos, Andreas, et al. "COVYT: Introducing the Coronavirus YouTube and TikTok speech dataset featuring the same speakers with and without infection." arXiv preprint arXiv:2206.11045 (2022). 39. https://datascience.nih.gov/strides 40. Radford, Alec, et al. "Robust speech recognition via large-scale weak supervision." International Conference on Machine Learning. PMLR, 2023. 41. Hardavella, Georgia, et al. "Top tips to deal with challenging situations: doctor–patient interactions." Breathe 13.2 (2017): 129-135. 42. Fairbanks, G. (1960). Voice and articulation drillbook, 2nd edn.  43. Nam, Yunyoung, Bersain A. Reyes, and Ki H. Chon. "Estimation of respiratory rates using the built-in microphone of a smartphone or headset." IEEE journal of biomedical and health informatics 20.6 (2015): 1493-1501. 44. Cahyawijaya, Samuel, Holy Lovenia, and Pascale Fung. "LLMs Are Few-Shot In-Context Low-Resource Language Learners." arXiv preprint arXiv:2403.16512 (2024). 45. Nguyen, Van Thai, et al. "Normative nasalance scores for Vietnamese-speaking children." Logopedics Phoniatrics Vocology 44.2 (2019): 51-57 46. Abaza, Mona M., et al. "Effects of medications on the voice." Otolaryngologic Clinics of North America 40.5 (2007): 1081-1090. 47. Vashani, K., et al. "Effectiveness of voice therapy in reflux-related voice disorders." Diseases of the Esophagus 23.1 (2010): 27-32. 48. Evans, Sarah, et al. "The relationship between testosterone and vocal frequencies in human males." Physiology & Behavior 93.4-5 (2008): 783-788. 49. Goldshtein, Evgenia, Ariel Tarasiuk, and Yaniv Zigel. "Automatic detection of obstructive sleep apnea using speech signals." IEEE Transactions on biomedical engineering 58.5 (2010): 1373-1382. 50. Solé-Casals, Jordi, et al. "Detection of severe obstructive sleep apnea through voice analysis." Applied Soft Computing 23 (2014): 346-354. 